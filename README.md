# üì¢ Release Notes

*Deployed: ‚Ä¢ Status: **Em produ√ß√£o***

---

## üöÄ Destaques ‚Äî **Novas Features**

|                                           | Feature           | Valor Gerado                                                                                                                                                                                                                          |
| ----------------------------------------- | ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| üõ°Ô∏è **Guardrail de Frete > R\$ 1.000,00** | **Simulation-MS** | Valida√ß√£o de outliers de frete: quando o Seller retorna valor acima de **R\$ 1.000,00**, a RN classifica o evento, registra auditoria e aciona o fluxo definido (revis√£o/ fallback), evitando erros de integra√ß√£o e custos indevidos. |
| üí∏ **Guardrail de Pre√ßo ¬±60%**            | **Produto-MS**    | Prote√ß√£o contra pre√ßos an√¥malos: bloqueia ou roteia para revis√£o quando o pre√ßo enviado pelo Seller √© **>160%** ou **<40%** do pre√ßo original; dispara alerta operacional e trilha de auditoria.                                      |

---

## üì¶ Mudan√ßas por Microservi√ßo

### **OrderStatus-MS**

* üóÉÔ∏è **Cache para fluxo de cancelamento de pedido**

  * Evita recomputo e reprocesso em curto intervalo; respostas idempotentes a repetidos.

### **Seller-MS**

* üß≠ **Defini√ß√£o do fluxo de Cadastro EQ3**

  * Padr√µes, contratos e estados mapeados para onboarding conforme EQ3.

> *Obs.: As novas features de **Simulation-MS** e **Produto-MS** est√£o detalhadas em **Destaques**.*

---

## üîÑ Melhorias Cross-Service

* üß∞ **Novo template de logs**

  * **JSON estruturado** com `traceId`, `correlationId`, `service`, `endpoint`, `latencyMs`, `statusCode`.
  * **Mascaramento de PII** e padroniza√ß√£o de n√≠veis (`INFO` core, `DEBUG` fluxo).
  * Facilita troubleshooting e correla√ß√£o entre servi√ßos.
* üìà **Migra√ß√£o de dashboards ‚Äî Datadog Ita√∫**

  * Pain√©is por dom√≠nio (Pedido, Seller, Produto, Simulation) com lat√™ncia, taxa de erro, throughput e satura√ß√£o.
  * Alertas unificados, tags padronizadas e nomenclatura consistente.
 
‚è± Jornada & Core Time

Core time fixo (j√° discutido).

Reuni√µes apenas dentro do core time.

‚ÄúNo meeting day‚Äù (1x por semana sem reuni√µes).

Direito a 1h de foco ininterrupto/dia (Deep Work Block).

üí¨ Comunica√ß√£o

Usar Slack/Teams com regras:

Dentro do core time ‚Üí responder r√°pido.

Fora do core time ‚Üí comunica√ß√£o ass√≠ncrona, sem obriga√ß√£o imediata.

Definir canal √∫nico por tema (ex.: bugs, deploys, produto).

Sempre deixar decis√µes registradas no Jira/Confluence, para evitar perda de contexto.

üîç Qualidade & Entregas

PRs revisados por pelo menos 1 dev + QA quando aplic√°vel.

Tempo m√°ximo para revis√£o de PR ‚Üí at√© 24h √∫teis.

Defini√ß√£o de ‚ÄúDefinition of Ready‚Äù e ‚ÄúDefinition of Done‚Äù claros.

Automatiza√ß√£o m√≠nima para garantir qualidade (CI rodando testes antes do merge).

üéØ Engajamento & Performance

Daily curta (15 min) ‚Üí foco em bloqueios e alinhamentos, n√£o status report.

Checkpoint semanal r√°pido para falar de moral do time e engajamento (‚Äúretro light‚Äù).

M√©trica coletiva > m√©trica individual (time ganha junto, erra junto).

Acordo de feedback cont√≠nuo (n√£o s√≥ na retro).

üìö Aprendizado & Crescimento

1h por sprint dedicada a Tech Talks internas (qualquer dev apresenta algo).

Rota√ß√£o de pap√©is nas cerim√¥nias (um sprint o dev X facilita a daily, no outro √© o dev Y).

Pares diferentes em pair programming para aumentar conhecimento cruzado.

üöÄ Entregas & Deploys

Deploys s√≥ em hor√°rio definido (ex.: manh√£s), para evitar inc√™ndios no fim do dia.

Post-mortem leve para qualquer incidente cr√≠tico ‚Üí sem ca√ßa √†s bruxas, s√≥ aprendizado.

Documentar aprendizados relevantes no Confluence.

‚öñÔ∏è Bem-estar & Cultura

Respeito ao hor√°rio fora do core (sem press√£o em msg fora do hor√°rio).

Acordo de ‚Äúc√¢mera ligada em reuni√µes importantes‚Äù (planning, retro, review).

Reconhecimento p√∫blico (cada sprint algu√©m destaca algo positivo de outro membro).

Pequenos rituais de engajamento (ex.: ‚Äúmeme da sprint‚Äù no Slack, ou ‚Äúice breaker‚Äù na retro).

üëâ A ideia n√£o √© adotar todos de uma vez, mas levar a lista para o time e deixar que eles escolham juntos os mais relevantes. Isso cria senso de pertencimento e aumenta muito o engajamento.

Quer que eu monte isso em formato visual de mural/working agreement (ex.: quadro dividido em blocos com √≠cones e bullets), pronto para voc√™ colar no Confluence ou mostrar no pr√≥ximo planning?



import okhttp3.Interceptor;
import okhttp3.Response;
import okhttp3.HttpUrl;

import org.springframework.stereotype.Component;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.Semaphore;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;

import jakarta.annotation.PreDestroy;

/**
 * Interceptor que aplica fair-share de concorr√™ncia por host.
 * - Se 1 host estiver ativo: ele pode usar at√© GLOBAL_MAX.
 * - Se N hosts estiverem ativos: cada um recebe roughly GLOBAL_MAX / N.
 *
 * Observa√ß√µes:
 * - Usamos Semaphore com crescimento (release) ao subir o limite.
 *   Ao reduzir, deixamos "convergir" naturalmente (sem reducePermits)
 *   para evitar edge cases; o novo limite passa a valer nas pr√≥ximas aquisi√ß√µes.
 */
@Component
public class FairShareInterceptor implements Interceptor {

    // ---------- Par√¢metros ajust√°veis ----------
    private static final int GLOBAL_MAX = 1000;           // capacidade total por inst√¢ncia
    private static final int MIN_PER_HOST = 50;           // piso por host para n√£o "matar" parceiros frios
    private static final int MAX_PER_HOST = 1000;         // teto por host (igual ao GLOBAL_MAX, por default)
    private static final int REBALANCE_INTERVAL_SEC = 1;  // intervalo de rebalanceamento
    private static final int ACTIVE_WINDOW_SEC = 5;       // janela para considerar um host "ativo"
    // -------------------------------------------

    private final ConcurrentHashMap<String, HostLimiter> limiters = new ConcurrentHashMap<>();
    private final ScheduledExecutorService scheduler =
            Executors.newSingleThreadScheduledExecutor(r -> {
                Thread t = new Thread(r, "fairshare-rebalance");
                t.setDaemon(true);
                return t;
            });

    public FairShareInterceptor() {
        scheduler.scheduleAtFixedRate(this::rebalance, 0, REBALANCE_INTERVAL_SEC, TimeUnit.SECONDS);
        scheduler.scheduleAtFixedRate(this::cleanupStale, 30, 30, TimeUnit.SECONDS); // limpeza peri√≥dica
    }

    @Override
    public Response intercept(Chain chain) throws IOException {
        HttpUrl url = chain.request().url();
        String host = url.host();

        HostLimiter limiter = limiters.computeIfAbsent(host, h -> new HostLimiter());
        limiter.markActive();

        limiter.acquire();
        try {
            return chain.proceed(chain.request());
        } finally {
            limiter.release();
        }
    }

    private void rebalance() {
        long now = System.nanoTime();
        long activeWindowNanos = TimeUnit.SECONDS.toNanos(ACTIVE_WINDOW_SEC);

        List<Map.Entry<String, HostLimiter>> ativos = new ArrayList<>();
        for (Map.Entry<String, HostLimiter> e : limiters.entrySet()) {
            HostLimiter hl = e.getValue();
            if (hl.recentlyActive(now, activeWindowNanos)) {
                ativos.add(e);
            }
        }

        int n = Math.max(ativos.size(), 1);
        int fair = Math.max(MIN_PER_HOST, Math.min(MAX_PER_HOST, GLOBAL_MAX / n));

        for (Map.Entry<String, HostLimiter> e : ativos) {
            e.getValue().setLimit(fair);
        }
    }

    private void cleanupStale() {
        long now = System.nanoTime();
        long staleWindowNanos = TimeUnit.SECONDS.toNanos(ACTIVE_WINDOW_SEC * 6L); // 6x a janela ativa
        for (Map.Entry<String, HostLimiter> e : limiters.entrySet()) {
            if (!e.getValue().recentlyActive(now, staleWindowNanos)) {
                // remover limiters que ficaram muito tempo sem tr√°fego
                limiters.remove(e.getKey(), e.getValue());
            }
        }
    }

    @PreDestroy
    public void shutdown() {
        scheduler.shutdownNow();
    }

    // ----------------- Helper -----------------
    static class HostLimiter {
        private final Semaphore sem = new Semaphore(0, true);
        private final AtomicInteger limit = new AtomicInteger(0);
        private volatile long lastTouch = System.nanoTime();

        void markActive() {
            lastTouch = System.nanoTime();
        }

        /**
         * "Ativo" se tocado dentro da janela.
         */
        public boolean recentlyActive(long now, long windowNanos) {
            return (now - lastTouch) < windowNanos;
        }

        /**
         * Ajusta o limite de concorr√™ncia.
         * Aumentos liberam novos permits; redu√ß√µes n√£o ‚Äútomam de volta‚Äù permits j√° dispon√≠veis,
         * mas o novo limite passa a valer conforme os permits forem sendo consumidos.
         */
        public void setLimit(int newLimit) {
            int old = limit.getAndSet(newLimit);
            int delta = newLimit - old;
            if (delta > 0) {
                sem.release(delta);
            }
            // Se delta < 0, deixamos convergir naturalmente.
        }

        public void acquire() {
            try {
                sem.acquire();
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                // Se interrompido, negue a passagem para n√£o "furar" o limiter.
                throw new RuntimeException("Interrupted while acquiring fair-share permit", e);
            }
        }

        public void release() {
            sem.release();
        }
    }
}
